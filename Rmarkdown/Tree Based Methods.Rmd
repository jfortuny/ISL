---
title: "Chapter 8 - Tree-Based Methods"
author: "Jos√© Fortuny"
date: "June 1, 2015"
output: html_document
---

# 8.3 Lab: Decision Trees

The *tree* library is used to construct regression and classification trees. We also use the *ISLR* package for the sample data.

```{r}
library(tree)
library(ISLR)
attach(Carseats)
View(Carseats)
```

## 8.3.1 Fitting Classification Trees

For our (tree) analysis of Car Sales we will recode the sales variable as a binary variable, called High and add this variable to the Carseats dataset:

```{r}
High <- ifelse(Sales <= 8, "No", "Yes")
Carseats <- data.frame(Carseats, High)
```

Now fit a classification tree to explain High as a function of all other attributes in Carseats except Sales:

```{r}
tree.carseats <- tree(High ~ . -Sales, Carseats)
summary(tree.carseats)
```

Now we can use plot() to visualize the tree generated as well as the name of the tree to see a text based version of the tree:

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
tree.carseats
```

The misclassification error rate based on the training data is very low: 9%. But, in practice, we need to validate the model against test data, as follows:

```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- High[-train]
tree.carseats <- tree(High ~ . -Sales, Carseats, subset = train)
tree.predict <- predict(tree.carseats, Carseats.test, type = "class")
table(tree.predict,High.test)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
tree.carseats
```

Would pruning the tree produce better results? The function cv.tree() performs cross-validation to determine the optimal level of tree pruning/complexity. We use FUN=prune.misclass to let the model prune guided by misclassification error rather than the default (deviance).

```{r}
set.seed(3)
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
cv.carseats
```

\$dev is the cross validation error rate, whose minimum is at 50, and this corresponds to a tree with 9 nodes (\$size). Let's plot these results:

```{r}
par(mfrow=c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

We can now prune the tree to obtain the 9-node tree:

```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty=0)
```

How does this pruned tree perform against the test dataset?

```{r}
tree.pred <- predict(prune.carseats, Carseats.test, type = "class")
table(tree.pred, High.test)
```

## 8.3.2 Fitting Regression Trees

For this exercise we use the Boston dataset in the MASS library.

```{r}
library(MASS)
View(Boston)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)
```

We can use cross validation to see whether pruning will improve performance:

```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

This is a more complex tree. We can try to prune it:

```{r}
yhat <- predict(tree.boston, newdata = Boston[-train,])
boston.test <- Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat - boston.test)^2)
```
